# Douglous_Portfolio
This is My Data analysis Portfolio

# [Project 1: TMDb_Movie_Analysis: Project Overview](https://github.com/Douglous-Sobei/TMDb_Movie_Analysis)
> I chose The TMDb Movie Data Set for data analysis. It contains information about 10,000 movies collected from the Movie Database(TMDb), including user ratings and revenue.

> Dropped some values in the revenue and budget which were zeros, removed the duplicate values in the dataset, and also dropped the misssing values to get the data set fixed.

> In the Data Analysis process, I answered the following questions:
- Which movie has the highest vote rate?
- What is the bottom 8 movies by popularity?
<img src="/Images/bottom_8_movies.png">
- What is the top Six revenue movies?
<img src="/Images/top_6_revenue_movies.png">
- What is the effect of the film budget over time in the first 35 years?
<img src="/Images/the_degree_budget.png">
# [Project 2: Data_Wrangling: Project Overview](https://github.com/Douglous-Sobei/Data_Wrangling)

> Data wrangling is an important skill that people working with data have to aquire since the world's data isn't clean. To make our data clean, wrangling is essential. In any case we analyze, visualize, or model our data before wrangling, the outcome could be making mistakes. Therefore, wrangling is the best practice to reach our expections.

> In the project, did the following;
- Got data from an existing file (twitter-archive-enhanced.csv) Reading from csv file using pandas
- Downloaded a file from the internet (image-predictions.tsv) using requests
- Did a query of an API (tweet_json.txt) to get JSON object of all the tweet_ids using Tweepy
- Imported the data into my programming environment (Jupyter Notebook)
- I assessed the visually and programmatically for quality and tidiness issues and made a documentation
- Fixed the quality and tidiness issues
- Made a copy in file to test the change before applying to the main dataset.
- Made insights and visualizations
